{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvJWxoNttApW",
        "outputId": "2fd9bf90-b4ff-49d9-e001-5c44cc890ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.2.0-202306081546-py3-none-any.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->monai) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->monai) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
            "Installing collected packages: monai\n",
            "Successfully installed monai-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import monai\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from skimage.transform import resize\n",
        "from monai.networks.nets import unet\n",
        "from monai.networks.blocks import Warp, DVF2DDF\n",
        "from monai.config import USE_COMPILED\n",
        "from tqdm import tqdm\n",
        "from torch.nn import MSELoss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qQITphgcttoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwavZpTEoxpR"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) MONAI Consortium\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "from monai.utils import LossReduction\n",
        "\n",
        "\n",
        "def spatial_gradient(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculate gradients on single dimension of a tensor using central finite difference.\n",
        "    It moves the tensor along the dimension to calculate the approximate gradient\n",
        "    dx[i] = (x[i+1] - x[i-1]) / 2.\n",
        "    Adapted from:\n",
        "        DeepReg (https://github.com/DeepRegNet/DeepReg)\n",
        "\n",
        "    Args:\n",
        "        x: the shape should be BCH(WD).\n",
        "        dim: dimension to calculate gradient along.\n",
        "    Returns:\n",
        "        gradient_dx: the shape should be BCH(WD)\n",
        "    \"\"\"\n",
        "    slice_1 = slice(1, -1)\n",
        "    slice_2_s = slice(2, None)\n",
        "    slice_2_e = slice(None, -2)\n",
        "    slice_all = slice(None)\n",
        "    slicing_s, slicing_e = [slice_all, slice_all], [slice_all, slice_all]\n",
        "    while len(slicing_s) < x.ndim:\n",
        "        slicing_s = slicing_s + [slice_1]\n",
        "        slicing_e = slicing_e + [slice_1]\n",
        "    slicing_s[dim] = slice_2_s\n",
        "    slicing_e[dim] = slice_1 #slice_2_e\n",
        "    return (x[slicing_s] - x[slicing_e])# / 2.0\n",
        "\n",
        "\n",
        "class GradEnergyLoss(_Loss):\n",
        "    \"\"\"\n",
        "    Calculate the Grad energy based on first-order differentiation of pred using forward finite difference.\n",
        "\n",
        "    Adapted from:\n",
        "        DeepReg (https://github.com/DeepRegNet/DeepReg)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalize: bool = False, reduction: Union[LossReduction, str] = LossReduction.MEAN) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            normalize:\n",
        "                Whether to divide out spatial sizes in order to make the computation roughly\n",
        "                invariant to image scale (i.e. vector field sampling resolution). Defaults to False.\n",
        "            reduction: {``\"none\"``, ``\"mean\"``, ``\"sum\"``}\n",
        "                Specifies the reduction to apply to the output. Defaults to ``\"mean\"``.\n",
        "\n",
        "                - ``\"none\"``: no reduction will be applied.\n",
        "                - ``\"mean\"``: the sum of the output will be divided by the number of elements in the output.\n",
        "                - ``\"sum\"``: the output will be summed.\n",
        "        \"\"\"\n",
        "        super().__init__(reduction=LossReduction(reduction).value)\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, pred: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pred: the shape should be BCH(WD)\n",
        "\n",
        "        Raises:\n",
        "            ValueError: When ``self.reduction`` is not one of [\"mean\", \"sum\", \"none\"].\n",
        "\n",
        "        \"\"\"\n",
        "        if pred.ndim not in [3, 4, 5]:\n",
        "            raise ValueError(f\"Expecting 3-d, 4-d or 5-d pred, instead got pred of shape {pred.shape}\")\n",
        "        for i in range(pred.ndim - 2):\n",
        "            if pred.shape[-i - 1] <= 4:\n",
        "                raise ValueError(f\"All spatial dimensions must be > 4, got spatial dimensions {pred.shape[2:]}\")\n",
        "        if pred.shape[1] != pred.ndim - 2:\n",
        "            raise ValueError(\n",
        "                f\"Number of vector components, {pred.shape[1]}, does not match number of spatial dimensions, {pred.ndim-2}\"\n",
        "            )\n",
        "\n",
        "        # first order gradient\n",
        "        first_order_gradient = [spatial_gradient(pred, dim) for dim in range(2, pred.ndim)]\n",
        "\n",
        "        # spatial dimensions in a shape suited for broadcasting below\n",
        "        if self.normalize:\n",
        "            spatial_dims = torch.tensor(pred.shape, device=pred.device)[2:].reshape((1, -1) + (pred.ndim - 2) * (1,))\n",
        "\n",
        "        energy = 0\n",
        "\n",
        "        for dim in range(len(first_order_gradient)):\n",
        "          energy += first_order_gradient[dim] ** 2\n",
        "\n",
        "        if self.reduction == LossReduction.MEAN.value:\n",
        "            energy = torch.mean(energy)  # the batch and channel average\n",
        "        elif self.reduction == LossReduction.SUM.value:\n",
        "            energy = torch.sum(energy)  # sum over the batch and channel dims\n",
        "        elif self.reduction != LossReduction.NONE.value:\n",
        "            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are [\"mean\", \"sum\", \"none\"].')\n",
        "\n",
        "        return energy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrRQJ5oSXF_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e151a789-9af1-4c92-cf3b-ec28f504425e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "shape of x_train: (12665, 28, 28), y_train: (12665,)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "\n",
        "(x_train_load, y_train_load), (x_test_load, y_test_load) = mnist.load_data()\n",
        "\n",
        "\n",
        "digit_sel1 = 0\n",
        "digit_sel2 = 1\n",
        "\n",
        "# extract instances of the first digit\n",
        "x_train_digit1 = x_train_load[y_train_load == digit_sel1, ...]\n",
        "y_train_digit1 = y_train_load[y_train_load == digit_sel1]\n",
        "x_test_digit1 = x_test_load[y_test_load == digit_sel1, ...]\n",
        "y_test_digit1 = y_test_load[y_test_load == digit_sel1]\n",
        "\n",
        "# extract instances of the second digit\n",
        "x_train_digit2 = x_train_load[y_train_load == digit_sel2, ...]\n",
        "y_train_digit2 = y_train_load[y_train_load == digit_sel2]\n",
        "x_test_digit2 = x_test_load[y_test_load == digit_sel2, ...]\n",
        "y_test_digit2 = y_test_load[y_test_load == digit_sel2]\n",
        "\n",
        "# concatenate the instances and labels of both digits\n",
        "x_train = np.concatenate((x_train_digit1, x_train_digit2), axis=0)\n",
        "y_train = np.concatenate((y_train_digit1, y_train_digit2), axis=0)\n",
        "x_test = np.concatenate((x_test_digit1, x_test_digit2), axis=0)\n",
        "y_test = np.concatenate((y_test_digit1, y_test_digit2), axis=0)\n",
        "\n",
        "# shuffle the combined instances and labels\n",
        "combined_data = list(zip(x_train, y_train))\n",
        "np.random.shuffle(combined_data)\n",
        "x_train, y_train = zip(*combined_data)\n",
        "\n",
        "x_train= np.array(x_train)\n",
        "y_train= np.array(y_train)\n",
        "\n",
        "# let's get some shapes to understand what we loaded.\n",
        "print('shape of x_train: {}, y_train: {}'.format(x_train.shape, y_train.shape))\n",
        "\n",
        "fixed = np.float32(x_train[:200])\n",
        "#fixed=fixed[None,:,:]\n",
        "moving = np.float32(0*x_train[4])\n",
        "\n",
        "fixed = resize(fixed,(fixed.shape[0],28,28))\n",
        "\n",
        "NUM_PROTO = 2\n",
        "moving = np.zeros((NUM_PROTO,28,28))\n",
        "#0*resize(moving,(64,64))   #initialising the moving image to 0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def img_is_color(img):\n",
        "\n",
        "    if len(img.shape) == 3:\n",
        "        # Check the color channels to see if they're all the same.\n",
        "        c1, c2, c3 = img[:, : , 0], img[:, :, 1], img[:, :, 2]\n",
        "        if (c1 == c2).all() and (c2 == c3).all():\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def show_image_list(list_images, list_titles=None, list_cmaps=None, grid=True, num_cols=2, figsize=(20, 10), title_fontsize=30):\n",
        "    '''\n",
        "    Shows a grid of images, where each image is a Numpy array. The images can be either\n",
        "    RGB or grayscale.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    images: list\n",
        "        List of the images to be displayed.\n",
        "    list_titles: list or None\n",
        "        Optional list of titles to be shown for each image.\n",
        "    list_cmaps: list or None\n",
        "        Optional list of cmap values for each image. If None, then cmap will be\n",
        "        automatically inferred.\n",
        "    grid: boolean\n",
        "        If True, show a grid over each image\n",
        "    num_cols: int\n",
        "        Number of columns to show.\n",
        "    figsize: tuple of width, height\n",
        "        Value to be passed to pyplot.figure()\n",
        "    title_fontsize: int\n",
        "        Value to be passed to set_title().\n",
        "    '''\n",
        "\n",
        "    assert isinstance(list_images, list)\n",
        "    assert len(list_images) > 0\n",
        "    assert isinstance(list_images[0], np.ndarray)\n",
        "\n",
        "    if list_titles is not None:\n",
        "        assert isinstance(list_titles, list)\n",
        "        assert len(list_images) == len(list_titles), '%d imgs != %d titles' % (len(list_images), len(list_titles))\n",
        "\n",
        "    if list_cmaps is not None:\n",
        "        assert isinstance(list_cmaps, list)\n",
        "        assert len(list_images) == len(list_cmaps), '%d imgs != %d cmaps' % (len(list_images), len(list_cmaps))\n",
        "\n",
        "    num_images  = len(list_images)\n",
        "    num_cols    = min(num_images, num_cols)\n",
        "    num_rows    = int(num_images / num_cols) + (1 if num_images % num_cols != 0 else 0)\n",
        "\n",
        "    # Create a grid of subplots.\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "\n",
        "    # Create list of axes for easy iteration.\n",
        "    if isinstance(axes, np.ndarray):\n",
        "        list_axes = list(axes.flat)\n",
        "    else:\n",
        "        list_axes = [axes]\n",
        "\n",
        "    for i in range(num_images):\n",
        "\n",
        "        img    = list_images[i]\n",
        "        title  = list_titles[i] if list_titles is not None else 'Image %d' % (i)\n",
        "        #cmap   = list_cmaps[i] if list_cmaps is not None else (None if img_is_color(img) else 'gray')\n",
        "\n",
        "        im=list_axes[i].imshow(img, cmap='viridis', vmin=0, vmax=255)\n",
        "\n",
        "        plt.colorbar(im,fraction=0.046, pad=0.04)\n",
        "        list_axes[i].set_title(title, fontsize=title_fontsize)\n",
        "        list_axes[i].grid(grid)\n",
        "\n",
        "    for i in range(num_images, len(list_axes)):\n",
        "        list_axes[i].set_visible(False)\n",
        "\n",
        "\n",
        "    fig.tight_layout()\n",
        "    _ = plt.show()\n"
      ],
      "metadata": {
        "id": "JnK5mb630OV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nofixed=5\n",
        "list_images=[moving]\n",
        "name=['Fixed Image']\n",
        "list_titles=list(np.repeat(name,nofixed))\n",
        "\n",
        "for i in range(nofixed):\n",
        "  list_images.append(fixed[i])\n",
        "\n",
        "#show_image_list(list_images=list_images,\n",
        "#                list_titles=['Moving Image']+list_titles,\n",
        "#                num_cols=2,\n",
        "#                figsize=(10, 10),\n",
        "#                grid=False,\n",
        "#                title_fontsize=15)"
      ],
      "metadata": {
        "id": "IEzddXt4zXIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum=0\n",
        "for i in range(100):\n",
        "  sum+=fixed[i]\n",
        "\n",
        "avg_fixed=sum/100\n",
        "plt.imshow(avg_fixed,vmin=0, vmax=255)\n",
        "plt.colorbar(fraction=0.046, pad=0.04)\n",
        "plt.title('Average Image')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "pl88XqPkLquM",
        "outputId": "a60cf9ea-d9f2-4493-a77c-a7c5c909de27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Average Image')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "_qaQiiT6L-NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x)"
      ],
      "metadata": {
        "id": "VVAdFyDuJaIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=NUM_PROTO):\n",
        "        super(Net,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3,stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.lf = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv1(input)\n",
        "        output = self.relu1(output)\n",
        "\n",
        "        output = self.maxpool1(output)\n",
        "\n",
        "        output = self.conv2(output)\n",
        "        output = self.relu2(output)\n",
        "\n",
        "        output = output.view(-1, 32 * 32 * 24)\n",
        "\n",
        "        output = self.lf(output)\n",
        "\n",
        "        return output'''"
      ],
      "metadata": {
        "id": "NTWYaivlLwoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee99cb7a-f347-4684-ee3a-59930f48547c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass Net(nn.Module):\\n\\n    def __init__(self, num_classes=NUM_PROTO):\\n        super(Net,self).__init__()\\n\\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3,stride=1, padding=1)\\n        self.relu1 = nn.ReLU()\\n\\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\\n\\n        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\\n        self.relu2 = nn.ReLU()\\n\\n        self.lf = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\\n\\n    def forward(self, input):\\n        output = self.conv1(input)\\n        output = self.relu1(output)\\n\\n        output = self.maxpool1(output)\\n\\n        output = self.conv2(output)\\n        output = self.relu2(output)\\n\\n        output = output.view(-1, 32 * 32 * 24)\\n\\n        output = self.lf(output)\\n\\n        return output'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prototype_selection_net = Net().to('cuda')\n",
        "prototype_selection_net.train()\n",
        "\n",
        "def select_prototype(image):\n",
        "\n",
        "    #image = torch.from_numpy(image)\n",
        "\n",
        "    # Predict the digit using the MNIST model\n",
        "    #with torch.no_grad():\n",
        "    prediction = prototype_selection_net(image)\n",
        "    predicted_digit = torch.argmax(prediction,dim=1)\n",
        "\n",
        "    return predicted_digit, prediction"
      ],
      "metadata": {
        "id": "vJ10ILSxKocX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digit_sel = 1\n",
        "x_train = x_train_load[y_train_load == digit_sel, ...]\n",
        "x_train = np.float32(x_train)\n",
        "x_train = x_train[:10,None,:,:]\n",
        "\n",
        "\n",
        "from skimage.transform import resize\n",
        "\n",
        "image = resize(x_train, (x_train.shape[0],1,28, 28))\n",
        "\n",
        "print(image.shape)\n",
        "#image = np.float32(x_train)\n",
        "\n",
        "# Classify the image using the MNIST model\n",
        "predicted_digit,prediction = select_prototype(torch.from_numpy(image).to('cuda'))\n",
        "\n",
        "print(f\"The image represents the digit: {predicted_digit}\")\n"
      ],
      "metadata": {
        "id": "-Hrj2BL1Qj0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664b1190-4862-4f34-d1d9-4fad66473cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1, 28, 28)\n",
            "The image represents the digit: tensor([0, 2, 4, 4, 4, 4, 3, 2, 4, 2], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-18859b8ec630>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(image.shape)\n",
        "print(prediction.shape)\n",
        "predicted_digit"
      ],
      "metadata": {
        "id": "nQnK4Ih-VGbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e979804-2690-4d35-f1e3-2947259636a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1, 28, 28)\n",
            "torch.Size([10, 10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 2, 4, 4, 4, 4, 3, 2, 4, 2], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v3tuYs_aATf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639f255a-8ced-402a-bd8a-178a643208c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/monai/networks/blocks/warp.py:67: UserWarning: monai.networks.blocks.Warp: Using PyTorch native grid_sample.\n",
            "  warnings.warn(\"monai.networks.blocks.Warp: Using PyTorch native grid_sample.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model()"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "reg = unet.UNet(spatial_dims=2,  # spatial dims\n",
        "    in_channels=2,\n",
        "    out_channels=2,# output channels (to represent 2D displacement vector field)\n",
        "    channels=(16, 32, 32, 32, 32),  # channel sequence\n",
        "    strides=(1, 2, 2, 4),  # convolutional strides\n",
        "    dropout=0.2,\n",
        "    norm=\"batch\").to('cuda')\n",
        "\n",
        "if USE_COMPILED:\n",
        "    warp_layer = Warp(2, padding_mode=\"zeros\").to('cuda')\n",
        "else:\n",
        "    warp_layer = Warp(\"bilinear\", padding_mode=\"zeros\").to('cuda')\n",
        "\n",
        "dvf_2_ddf = DVF2DDF()\n",
        "\n",
        "reg.train()\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, img):\n",
        "        super(Model, self).__init__()\n",
        "        self.moving_img = nn.Parameter(img)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.moving_img\n",
        "\n",
        "\n",
        "moving_net = Model(torch.tensor(0*moving)).to('cuda')\n",
        "\n",
        "optimizerR = torch.optim.Adam(reg.parameters(), lr=0.01)\n",
        "optimizerM = torch.optim.Adam(moving_net.parameters(), lr=0.1)\n",
        "optimizerP = torch.optim.Adam(prototype_selection_net.parameters(), lr=0.1)\n",
        "\n",
        "moving_net.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQJGNZ6DfVyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dbfaa6b-1b1c-4413-c008-2bf83b02aaef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MSELoss()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "image_loss = MSELoss()\n",
        "regularization_loss = GradEnergyLoss()\n",
        "image_loss.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e15x5HlWBA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b6288d-7b07-429a-e3e4-5c99521eddca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 28, 28)\n",
            "(200, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(moving.shape)\n",
        "print(fixed.shape)\n",
        "fixedo = fixed\n",
        "movingo=moving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP-R8_hTexy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68f8fce-d665-46fe-9b3f-6dcdacbffa11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "moving = moving_net()\n",
        "\n",
        "print(moving.shape)\n",
        "\n",
        "\n",
        "#print(torch.tile(moving[None,:,:,:],(fixed.shape[0],1,1,1)).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtF7X5pmdmlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7088bbb5-9156-45bc-850a-54e4aac13d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50000 [00:00<?, ?it/s]<ipython-input-9-18859b8ec630>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x)\n",
            "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-295e066d4abe>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#moving = torch.tile(moving1[None,None,:,:],(fixed.shape[0],1,1,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmoving\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#input_data = torch.cat((moving[:,None,:,:], fixed), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "fixed = fixedo[:,None,:,:]\n",
        "#moving = np.tile(movingo,(fixed.shape[0],1,1,1))\n",
        "\n",
        "max_epochs = 50000\n",
        "\n",
        "loss_array=[]\n",
        "epoch_array=[]\n",
        "\n",
        "fixed = torch.tensor(fixed).to('cuda')\n",
        "\n",
        "for epoch in tqdm(range(max_epochs)):\n",
        "\n",
        "    optimizerM.zero_grad()\n",
        "   # optimizerR.zero_grad()\n",
        "    optimizerP.zero_grad()\n",
        "\n",
        "    prototype_num, _ = select_prototype(fixed)\n",
        "\n",
        "\n",
        "\n",
        "    moving1 = moving_net()\n",
        "    moving = moving1[prototype_num,:,:]\n",
        "\n",
        "\n",
        "    #moving = torch.tile(moving1[None,None,:,:],(fixed.shape[0],1,1,1))\n",
        "    moving = moving.float()\n",
        "\n",
        "    #input_data = torch.cat((moving[:,None,:,:], fixed), dim=1)\n",
        "    #dvf = reg(input_data)\n",
        "    #ddf = dvf_2_ddf(dvf)\n",
        "    moved = moving[:,None,:,:] #warp_layer(moving[:,None,:,:], ddf)\n",
        "\n",
        "    imgloss = image_loss(moved, fixed)# + 300000*regularization_loss(ddf)\n",
        "\n",
        "    imgloss.backward()\n",
        "\n",
        "    optimizerP.step()\n",
        "    #optimizerR.step()\n",
        "    optimizerM.step()\n",
        "\n",
        "    loss_value=imgloss.item()\n",
        "    loss_array.append(loss_value)\n",
        "    epoch_array.append(epoch)\n",
        "    #print(f\"Epoch: {epoch}, Loss: {loss_value}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moving1 = moving_net()\n",
        "moving1=moving1.detach().to('cpu').numpy()\n",
        "plt.imshow(moving1[0],vmin=0, vmax=200)"
      ],
      "metadata": {
        "id": "u_mBF_MGNd-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(moving1[1],vmin=0, vmax=200)"
      ],
      "metadata": {
        "id": "GcPNXksEPr-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(moving1[2],vmin=0, vmax=200)"
      ],
      "metadata": {
        "id": "-aAxpqyygOQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_images= [moving.detach()[0,0].to('cpu').numpy(),\n",
        "               fixed[0,0].to('cpu').numpy(),\n",
        "               moved[0,0].detach().to('cpu').numpy(),\n",
        "               ddf[0,0].detach().to('cpu').numpy(),\n",
        "               ddf[0,1].detach().to('cpu').numpy()]\n",
        "show_image_list(list_images=output_images,\n",
        "                list_titles=['Learnt Moving Image','Fixed Image','Moved Image','Deformation Field','Deformation Field'],\n",
        "                num_cols=3,\n",
        "                figsize=(10, 10),\n",
        "                grid=False,\n",
        "                title_fontsize=15)"
      ],
      "metadata": {
        "id": "WG8TSEbUrIPa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}